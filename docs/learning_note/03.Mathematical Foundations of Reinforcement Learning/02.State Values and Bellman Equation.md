---
title: State Values and Bellman Equation
createTime: 2024-05-15 22:15:05
permalink: /mfrl/823c00/
categories:
  - 学习笔记
  - Mathematical Foundations of Reinforcement Learning
tags:
  - 
---

这章介绍的东西当初看着很多,现在回看回来其实就介绍了几个基础概念

## state values

在状态 S 下, 所能取得的 return 的合,就可以简单的理解为 state value，在确定的策略下，就等于（s,a）下这action value，在stomatic下就是乘一下策略的概率分布。

## Bellman Equation

$$
\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]=\left[\begin{array}{l}
r_\pi\left(s_1\right) \\
r_\pi\left(s_2\right) \\
r_\pi\left(s_3\right) \\
r_\pi\left(s_4\right)
\end{array}\right]+\gamma\left[\begin{array}{llll}
p_\pi\left(s_1 \mid s_1\right) & p_\pi\left(s_2 \mid s_1\right) & p_\pi\left(s_3 \mid s_1\right) & p_\pi\left(s_4 \mid s_1\right) \\
p_\pi\left(s_1 \mid s_2\right) & p_\pi\left(s_2 \mid s_2\right) & p_\pi\left(s_3 \mid s_2\right) & p_\pi\left(s_4 \mid s_2\right) \\
p_\pi\left(s_1 \mid s_3\right) & p_\pi\left(s_2 \mid s_3\right) & p_\pi\left(s_3 \mid s_3\right) & p_\pi\left(s_4 \mid s_3\right) \\
p_\pi\left(s_1 \mid s_4\right) & p_\pi\left(s_2 \mid s_4\right) & p_\pi\left(s_3 \mid s_4\right) & p_\pi\left(s_4 \mid s_4\right)
\end{array}\right]\left[\begin{array}{l}
v_\pi\left(s_1\right) \\
v_\pi\left(s_2\right) \\
v_\pi\left(s_3\right) \\
v_\pi\left(s_4\right)
\end{array}\right]
$$

这是我觉得最直观的贝尔曼方程的向量表现形式了，虽然每个 V 都依赖其他状态的 V，但整体作为向量却可以得到一个解，体现了数学的美。
