---
title: from ppo to mappo
createTime: 2025/02/25 17:40:07
permalink: /learn/hoRL/rfasfo03/
---

## IPPO 与 MAPPO 的不同

在 hands-on-rl 这书中, 采用了共享参数的方式来训练多个 agent。

这时候，唯一的不同就是 critic 的网络了，在 IPPO 中，critic 网络接收的是每一个 agent 单独的 observation，而在 MAPPO 中，critic 网络接收的是所有 agent 的 observation。

## 多智能体的归一化是怎么做的

单智能体的归一化只用对单个智能体的数据进行归一化即可，
但是多智能体的归一化 要怎么对待呢？
是每个智能体单独归一化，还是所有智能体一起归一化呢？

就我理解, 归一化是要保证数据的波动不是很大, 而且所有的 agent 都是接的一个 actor 网络
所以是所有智能体一起归一化。

## light ppo 改动

light ppo 输出的动作是 tuple(list(int),list(int)) ,但是ma-gym 要的是 list(int) ,所以要改动一下。

### actor

原文件在 `/algorithms/utils/mlp`

有意思的一点在于 fc2 的构件, 他是一个 相同 layer 的复制, 通过 loop layerN 来快速构建重复的 layer
同时让不同的 layer 参数不同

```python
    self.fc_h = nn.Sequential(
        init_(nn.Linear(hidden_size, hidden_size)),
        active_func,
        nn.LayerNorm(hidden_size),
    )
    self.fc2 = get_clones(self.fc_h, self._layer_N)

def forward(self, x):
    x = self.fc1(x)
      # 默认为 1
    for i in range(self._layer_N):
        x = self.fc2[i](x)
    return x
```

### train

每个 agent 都有一个 actor
先pre train 再 train

```python
    for agent_id in range(self._n_agents):
        self._actor[agent_id].train()
        self._actor[agent_id].pre_train()
```

### share obs

把每个 agent 的 obs 连了起来,像是 concat 一样

