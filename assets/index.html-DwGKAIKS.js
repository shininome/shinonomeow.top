import{_ as t,c as e,b as i,a as s,o as n}from"./app-BtfO1hh9.js";const l={};function p(h,a){return n(),e("div",null,a[0]||(a[0]=[i(`<p>开始了文本方向的神经网络，之前的卷积看完还是很不知所云的， 对于为什么这样会更好还是只有一个大概的解释，估计文本这方面也不遑多让吧。</p><p>I have started learing language Models, but I&#39;m still confused after studying Convolutional Neural Networks. The explanations provided are quite vague, and I suspect Language Models might be just as challenging as CNNs</p><p>这一节从马可夫链开始，到一个简单的 RNN 实现结束，主要的难点还是在 RNN 实现方面的细节。</p><p>This section begins from Markov Chain, and concludes with a basic implementation of an RNN from scratch. The main challenge lies in the detailed of implementation of the RNN.</p><h2 id="循环神经网络从零开始实现-rnn-scratch" tabindex="-1"><a class="header-anchor" href="#循环神经网络从零开始实现-rnn-scratch"><span>循环神经网络从零开始实现 / rnn scratch</span></a></h2><h3 id="数据格式-data-format" tabindex="-1"><a class="header-anchor" href="#数据格式-data-format"><span>数据格式 / data format</span></a></h3><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">train_iter</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> vocab </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> load_data_time_machine</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">batch_size</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> num_steps</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div></div></div><div class="hint-container note"><p class="hint-container-title">注</p><p>train_iter: 每次输出一个批量大小的 X 和 Y 。</p><p>train_iter: ouput a batch of X and Y with each iteration</p><p>X: tensor([[13, 14, 15, 16, 17], [28, 29, 30, 31, 32]])</p><p>Y: tensor([[14, 15, 16, 17, 18], [29, 30, 31, 32, 33]])</p><p>X: tensor([[ 3, 4, 5, 6, 7], [18, 19, 20, 21, 22]])</p><p>Y: tensor([[ 4, 5, 6, 7, 8], [19, 20, 21, 22, 23]])</p><p>X: tensor([[ 8, 9, 10, 11, 12], [23, 24, 25, 26, 27]])</p><p>Y: tensor([[ 9, 10, 11, 12, 13], [24, 25, 26, 27, 28]])</p></div><h3 id="为什么要对-x-进行转置-why-should-we-transpose-tensor-x" tabindex="-1"><a class="header-anchor" href="#为什么要对-x-进行转置-why-should-we-transpose-tensor-x"><span>为什么要对 X 进行转置 / why should we transpose tensor X</span></a></h3><p>首先需要知道的是本节所有的预测都是对<mark>字符</mark>的预测，所以 one_hot 的分类数是 28 。</p><p>Firstly, it is important to note that all predictions in this section are predictions of characters, so the number of class for the one-hot encoding is 28.</p><div class="hint-container note"><p class="hint-container-title">注</p><p>26 个英文字母 + 空格 + unk</p><p>26 English characters + space + unk</p></div><div class="language-python line-numbers-mode" data-highlighter="shiki" data-ext="python" style="--shiki-light:#393a34;--shiki-dark:#dbd7caee;--shiki-light-bg:#ffffff;--shiki-dark-bg:#121212;"><pre class="shiki shiki-themes vitesse-light vitesse-dark vp-code"><code><span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span></span>
<span class="line"><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">from</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">nn </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">import</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> functional </span><span style="--shiki-light:#1E754F;--shiki-dark:#4D9375;">as</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> F</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> torch</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">arange</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">10</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">).</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">reshape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;">2</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 5</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">ans </span><span style="--shiki-light:#999999;--shiki-dark:#666666;">=</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;"> F</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">one_hot</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">X</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">,</span><span style="--shiki-light:#2F798A;--shiki-dark:#4C9A91;"> 28</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">ans</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span>
<span class="line"><span style="--shiki-light:#998418;--shiki-dark:#B8A965;">print</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">(</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">ans</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">.</span><span style="--shiki-light:#393A34;--shiki-dark:#DBD7CAEE;">shape</span><span style="--shiki-light:#999999;--shiki-dark:#666666;">)</span></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>每次做的是对一个批量进行预测，所以每次我们取的是</p><p>Since our predictions are conducted on batches, our selection contain</p>`,15),s("p",{class:"katex-block"},[s("span",{class:"katex-display"},[s("span",{class:"katex"},[s("span",{class:"katex-mathml"},[s("math",{xmlns:"http://www.w3.org/1998/Math/MathML",display:"block"},[s("semantics",null,[s("mrow",null,[s("mo",{fence:"true"},"["),s("mtable",{rowspacing:"0.16em",columnalign:"center center",columnspacing:"1em"},[s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mn",null,"0")])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mn",null,"1")])])]),s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mn",null,"1")])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mn",null,"2")])])]),s("mtr",null,[s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mn",null,"2")])]),s("mtd",null,[s("mstyle",{scriptlevel:"0",displaystyle:"false"},[s("mn",null,"3")])])])]),s("mo",{fence:"true"},"]")]),s("annotation",{encoding:"application/x-tex"},"\\begin{bmatrix} 0&1\\\\ 1&2\\\\ 2&3\\\\ \\end{bmatrix} ")])])]),s("span",{class:"katex-html","aria-hidden":"true"},[s("span",{class:"base"},[s("span",{class:"strut",style:{height:"3.6em","vertical-align":"-1.55em"}}),s("span",{class:"minner"},[s("span",{class:"mopen"},[s("span",{class:"delimsizing mult"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"2.05em"}},[s("span",{style:{top:"-4.05em"}},[s("span",{class:"pstrut",style:{height:"5.6em"}}),s("span",{style:{width:"0.667em",height:"3.600em"}},[s("svg",{xmlns:"http://www.w3.org/2000/svg",width:"0.667em",height:"3.600em",viewBox:"0 0 667 3600"},[s("path",{d:`M403 1759 V84 H666 V0 H319 V1759 v0 v1759 h347 v-84
H403z M403 1759 V0 H319 V1759 v0 v1759 h84z`})])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.55em"}},[s("span")])])])])]),s("span",{class:"mord"},[s("span",{class:"mtable"},[s("span",{class:"col-align-c"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"2.05em"}},[s("span",{style:{top:"-4.21em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"0")])]),s("span",{style:{top:"-3.01em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])]),s("span",{style:{top:"-1.81em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.55em"}},[s("span")])])])]),s("span",{class:"arraycolsep",style:{width:"0.5em"}}),s("span",{class:"arraycolsep",style:{width:"0.5em"}}),s("span",{class:"col-align-c"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"2.05em"}},[s("span",{style:{top:"-4.21em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"1")])]),s("span",{style:{top:"-3.01em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"2")])]),s("span",{style:{top:"-1.81em"}},[s("span",{class:"pstrut",style:{height:"3em"}}),s("span",{class:"mord"},[s("span",{class:"mord"},"3")])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.55em"}},[s("span")])])])])])]),s("span",{class:"mclose"},[s("span",{class:"delimsizing mult"},[s("span",{class:"vlist-t vlist-t2"},[s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"2.05em"}},[s("span",{style:{top:"-4.05em"}},[s("span",{class:"pstrut",style:{height:"5.6em"}}),s("span",{style:{width:"0.667em",height:"3.600em"}},[s("svg",{xmlns:"http://www.w3.org/2000/svg",width:"0.667em",height:"3.600em",viewBox:"0 0 667 3600"},[s("path",{d:`M347 1759 V0 H0 V84 H263 V1759 v0 v1759 H0 v84 H347z
M347 1759 V0 H263 V1759 v0 v1759 h84z`})])])])]),s("span",{class:"vlist-s"},"​")]),s("span",{class:"vlist-r"},[s("span",{class:"vlist",style:{height:"1.55em"}},[s("span")])])])])])])])])])])],-1),i('<p>这里将X进行转置后相邻的批次读取的是<mark>时间</mark>上连续的，也就是文本顺序是正确的。</p><p>We transpose X to ensure that consecutive batches are read in time order, meaning the text sequence is pre preserved</p><h3 id="转置的好处-the-benefits-of-transposition" tabindex="-1"><a class="header-anchor" href="#转置的好处-the-benefits-of-transposition"><span>转置的好处 / the benefits of transposition</span></a></h3><p>其实就是一个行优先和列优先的问题，以及个人的习惯。</p><p>this a column provity or row proviet, and personal habbits this is actually a question of row priority versus column priority, as well as individual habits</p><p>python 是行优先的，所以每次批量读取转值后会带来 cache 命中的提升。</p><p>Python follows row priority, so each batch transpositon will result in improvement in cache hit rate</p><h2 id="torch-mm" tabindex="-1"><a class="header-anchor" href="#torch-mm"><span>torch.mm</span></a></h2><p>还是看GPT吧</p><p>torch.mm() 和 torch.matmul() 是 PyTorch 中用于执行矩阵乘法的两个函数，它们有一些区别：</p><h3 id="输入类型" tabindex="-1"><a class="header-anchor" href="#输入类型"><span>输入类型：</span></a></h3><p>torch.mm() 只能接受二维张量作为输入，即矩阵。</p><p>torch.matmul() 可以接受张量的任意维度作为输入，因此可以用于更广泛的矩阵乘法操作，包括批量矩阵乘法、广播等。</p><h3 id="广播规则" tabindex="-1"><a class="header-anchor" href="#广播规则"><span>广播规则：</span></a></h3><p>torch.mm() 对输入张量进行严格的形状匹配，要求两个输入张量的形状都是二维，并且第一个张量的列数必须等于第二个张量的行数。</p><p>torch.matmul() 则遵循广播规则，可以在满足一定条件的情况下，对具有不同形状的张量进行乘法操作。例如，可以对两个三维张量进行乘法，其中第一个张量的最后两个维度的形状必须与第二个张量的倒数两个维度的形状相匹配。</p><h3 id="支持批量矩阵乘法" tabindex="-1"><a class="header-anchor" href="#支持批量矩阵乘法"><span>支持批量矩阵乘法：</span></a></h3><p>torch.mm() 不支持批量矩阵乘法，即一次性处理多个矩阵乘积。</p><p>torch.matmul() 可以通过在输入张量的前面添加额外的维度来支持批量矩阵乘法，这在处理多个样本或批次数据时非常有用。</p><h2 id="有关-y-label" tabindex="-1"><a class="header-anchor" href="#有关-y-label"><span>有关 Y/label</span></a></h2><p>Y是一个 2x5 的 tensor ， rnn输出的是一个 10x28 的 tensor，这也就意味着 loss 那里要做一个 reshape。</p><h3 id="rnn-细节" tabindex="-1"><a class="header-anchor" href="#rnn-细节"><span>rnn 细节</span></a></h3><p>output 是一个 len 为 5 的 list 。list 里面为 torch.size(2,28) 的tensor。</p><p>对 list 进行 cat dim = 0 。</p>',24)]))}const o=t(l,[["render",p]]),c=JSON.parse(`{"path":"/learn/d2l/471b4f/","title":"循环神经网络","lang":"zh-CN","frontmatter":{"title":"循环神经网络","createTime":"2024-04-25T09:51:03.000Z","permalink":"/learn/d2l/471b4f/","categories":["学习笔记","动手深度学习"],"tags":["RNN"],"feed":{"enable":true},"description":"开始了文本方向的神经网络，之前的卷积看完还是很不知所云的， 对于为什么这样会更好还是只有一个大概的解释，估计文本这方面也不遑多让吧。 I have started learing language Models, but I'm still confused after studying Convolutional Neural Networks. T...","head":[["script",{"type":"application/ld+json"},"{\\"@context\\":\\"https://schema.org\\",\\"@type\\":\\"Article\\",\\"headline\\":\\"循环神经网络\\",\\"image\\":[\\"\\"],\\"dateModified\\":\\"2025-01-27T11:13:57.000Z\\",\\"author\\":[]}"],["meta",{"property":"og:url","content":"https://shinonomoew.top/learn/d2l/471b4f/"}],["meta",{"property":"og:site_name","content":"東雲研究所"}],["meta",{"property":"og:title","content":"循环神经网络"}],["meta",{"property":"og:description","content":"开始了文本方向的神经网络，之前的卷积看完还是很不知所云的， 对于为什么这样会更好还是只有一个大概的解释，估计文本这方面也不遑多让吧。 I have started learing language Models, but I'm still confused after studying Convolutional Neural Networks. T..."}],["meta",{"property":"og:type","content":"article"}],["meta",{"property":"og:locale","content":"zh-CN"}],["meta",{"property":"og:updated_time","content":"2025-01-27T11:13:57.000Z"}],["meta",{"property":"article:tag","content":"RNN"}],["meta",{"property":"article:modified_time","content":"2025-01-27T11:13:57.000Z"}]]},"readingTime":{"minutes":3.4,"words":1021},"git":{"createdTime":1737976437000,"updatedTime":1737976437000,"contributors":[{"name":"東雲柊","username":"","email":"gtx2shino@gmail.com","commits":1,"avatar":"https://gravatar.com/avatar/a85c5cf42a533c53c895373652a8b15621ed6ecb8411d8e1b58c894a5ac91de9?d=retro"}]},"autoDesc":true,"filePathRelative":"learning_note/01.dive_into_deeplearning/08.循环神经网络/08.recurrent_neural_Network.md","headers":[],"categoryList":[{"id":"987a5d","sort":10001,"name":"learning_note"},{"id":"35f7ea","sort":1,"name":"dive_into_deeplearning"},{"id":"fea38a","sort":8,"name":"循环神经网络"}]}`);export{o as comp,c as data};
